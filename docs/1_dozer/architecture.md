---
sidebar_position: 2
---

# Architecture

## Node Types
Dozer's architecture is designed to efficiently process and deliver data through three types of nodes: Ingestor nodes, Processor nodes, and Store/API nodes. 

**Ingestor Nodes**: These nodes are responsible for data ingestion from various sources such as databases, data warehouses, or object storages. They capture data continuously in real-time using Change Data Capture (CDC) or near-real-time using polling mechanisms. The captured data is transformed into Dozer operations (Inserts, Updates, Deletes) and ingested into the system.

**Processor Nodes**: These nodes embody a streaming SQL engine that enables real-time data transformations. As data is ingested, these transformations operate on the incoming streams directly. Processor nodes can source data from multiple ingestors or other processors, and they can combine and aggregate data using SQL.

**Store/API Nodes**: Once data is processed, it's transferred to the Store nodes. These nodes implement a data store using LMDB (Lightning Memory-Mapped Database), an ultra-fast, ultra-compact key-value embedded data store. The stored data is automatically indexed to expedite lookup performance and is accessible through gRPC and REST APIs. The definitions of the exposed data are available through OpenAPI or Protocol Buffers definitions.

![Dozer Architecture](./arch.svg)

## Data Flows

![Dozer Architecture](./images/e2e.svg)

### Ingestor Nodes
Ingestor nodes form the initial data pipeline by connecting to various data sources and streaming data into the system. Each Ingestor node maintains an in-memory queue of all incoming messages, allowing for high-speed data processing. However, this queue has a size limit to prevent memory overflow. 

When the volume of incoming data reaches a certain threshold, the Ingestor node initiates an offloading process. The earliest (or head) messages in the queue, which are likely to have been processed already, are moved to a cloud storage system. This mechanism of transferring older data to more permanent storage allows the Ingestor node to free up memory space for new incoming data, ensuring smooth, uninterrupted data flow and real-time processing.

Ingestor nodes not only process incoming data but also serve as crucial data access points for the rest of the system. They achieve this by exposing a gRPC endpoint that downstream nodes can connect to. This gRPC endpoint implements a protocol to provide access to data for all downstream nodes, effectively allowing for the distribution and dissemination of data across the system.

### Processor Nodes
Processor nodes in Dozer play a vital role in executing transformations and managing data flow. They translate any SQL query into a streaming Direct Acyclic Graph (DAG), enabling real-time transformation execution. To access the data, these Processor nodes form a gRPC streaming connection with upstream nodes, which could be either Ingestor nodes or other Processor nodes. Like their Ingestor counterparts, Processor nodes also expose a gRPC endpoint to facilitate data access for downstream nodes.

To manage the data generated by transformations, Processor nodes maintain a queue system similar to the one employed by Ingestor nodes. A portion of this data queue is kept in memory, while the remainder is offloaded to cloud storage as needed to maintain optimal memory usage.

In addition to data queue management, Processor nodes maintain a state for data processing. This state is preserved in memory and snapshotted to cloud storage at regular intervals to ensure data integrity. The snapshotting process utilizes a variation of the Lamport-Candy algorithm to ensure consistency.

Each Processor node hosts numerous micro-nodes, each responsible for executing individual operations. For instance, if a query involves joining several data sources, filtering, and aggregations, each operation is assigned to an individual micro-node. These micro-nodes each run in their own thread, with data passed from one micro-node to the next in a pipeline fashion. This approach maximizes efficiency when running on multi-core processors, ensuring fast and reliable data processing.

![Dozer Architecture](./images/proc_node_start.svg)

When a new Processor node is instantiated in Dozer, several key steps occur to initialize it and prepare it for operation:

1. **DAG Transformation**: The incoming SQL is converted into a Direct Acyclic Graph (DAG), and all micro-nodes are initialized for operation. This process sets the stage for efficient and organized data processing.

2. **Connection to Upstream Nodes**: The Processor node establishes connections with all necessary upstream nodes and requests access to the ingested data. This connection forms the primary data pathway for the Processor node.

3. **Data Access from Upstream Nodes**: In response to the request from the Processor node, the upstream nodes provide both the location of offloaded data stored in cloud storage, as well as streaming their in-memory data to the downstream node. This process ensures the Processor node receives all necessary data for processing.

4. **Data Processing**: Upon receiving the data, the Processor node combines the offloaded data with the live data it receives. It starts processing this consolidated data set, generating a new data stream. This newly created data stream is partially kept in memory and partially offloaded to cloud storage, maintaining a balance for efficient resource usage.
